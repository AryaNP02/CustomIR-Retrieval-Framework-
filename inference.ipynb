{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9da35a",
   "metadata": {},
   "source": [
    "## Section 1: Project Structure Setup\n",
    "\n",
    "**Key Design**: Implementation logic is in `.py` files, configuration in YAML, and inference/evaluation in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364fb1a5",
   "metadata": {},
   "source": [
    "## Section 2: Import Implementation Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e10c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd()\n",
    "src_path = project_root / 'src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Source path: {src_path}\")\n",
    "print(f\"Source path exists: {src_path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all modules from src package\n",
    "from src import (\n",
    "    IndexInfo, DataStore, Compression, QueryProc, Optimizations,\n",
    "    CompressionUtils,\n",
    "    TextPreprocessor,\n",
    "    InvertedIndex,\n",
    "    BooleanExprParser, QueryProcessor,\n",
    "    MetricsCollector, Reporter,\n",
    "    IndexBuilder, TestQueryGenerator\n",
    ")\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "\n",
    "print(\"‚úì All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25f972",
   "metadata": {},
   "source": [
    "## Section 3: Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ddc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from YAML\n",
    "config_path = project_root / 'config' / 'index_config.yaml'\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"‚úì Configuration loaded successfully\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Config file not found at {config_path}\")\n",
    "    config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ab45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display loaded configuration\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADED CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Data configuration\n",
    "data_config = config.get('data', {})\n",
    "print(f\"\\nüìÅ Data Settings:\")\n",
    "print(f\"   Directory: {data_config.get('directory', 'N/A')}\")\n",
    "print(f\"   Max Docs: {data_config.get('max_docs', 'Unlimited')}\")\n",
    "\n",
    "# Index configuration\n",
    "index_config = config.get('index', {})\n",
    "print(f\"\\nüìä Index Settings:\")\n",
    "print(f\"   Type: {index_config.get('type', 'BOOLEAN')}\")\n",
    "print(f\"   Storage: {index_config.get('storage', 'CUSTOM')}\")\n",
    "print(f\"   Compression: {index_config.get('compression', 'NONE')}\")\n",
    "print(f\"   Optimization: {index_config.get('optimization', 'NULL')}\")\n",
    "print(f\"   Query Strategy: {index_config.get('query_strategy', 'TERM_AT_A_TIME')}\")\n",
    "\n",
    "# Query configuration\n",
    "query_config = config.get('query', {})\n",
    "print(f\"\\nüîç Query Settings:\")\n",
    "print(f\"   Default Top-K: {query_config.get('default_top_k', 10)}\")\n",
    "print(f\"   Max Results: {query_config.get('max_results', 1000)}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65180ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key parameters for easy access\n",
    "DATA_DIR = Path(data_config.get('directory', 'free-news-datasets/News_Datasets'))\n",
    "MAX_DOCS = int(data_config.get('max_docs', 1e6))\n",
    "INDEX_TYPE = index_config.get('type', 'BOOLEAN')\n",
    "STORAGE_TYPE = index_config.get('storage', 'CUSTOM')\n",
    "COMPRESSION_TYPE = index_config.get('compression', 'NONE')\n",
    "OPTIMIZATION_TYPE = index_config.get('optimization', 'NULL')\n",
    "QUERY_STRATEGY = index_config.get('query_strategy', 'TERM_AT_A_TIME')\n",
    "DEFAULT_TOP_K = query_config.get('default_top_k', 10)\n",
    "\n",
    "print(f\"\\n‚úì Configuration parameters extracted\")\n",
    "print(f\"  DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"  MAX_DOCS: {MAX_DOCS}\")\n",
    "print(f\"  INDEX_TYPE: {INDEX_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4a7cf",
   "metadata": {},
   "source": [
    "## Section 4: Initialize Models and Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct version string from configuration\n",
    "# Format: SelfIndex-v1.xyziqQ\n",
    "\n",
    "# Map type names to codes\n",
    "index_type_map = {'BOOLEAN': '1', 'WORDCOUNT': '2', 'TFIDF': '3'}\n",
    "storage_map = {'CUSTOM': '1', 'SQLITE': '2', 'REDIS': '3'}\n",
    "compression_map = {'NONE': '1', 'VARBYTE_ENCODING': '2', 'GZIP_COMPRESSION': '3'}\n",
    "optimization_map = {'NULL': '0', 'SKIPPING': 'sp', 'THRESHOLDING': 'th', 'EARLY_STOPPING': 'es'}\n",
    "strategy_map = {'TERM_AT_A_TIME': 'T', 'DOCUMENT_AT_A_TIME': 'D'}\n",
    "\n",
    "x = index_type_map.get(INDEX_TYPE, '1')\n",
    "y = storage_map.get(STORAGE_TYPE, '1')\n",
    "z = compression_map.get(COMPRESSION_TYPE, '1')\n",
    "i = optimization_map.get(OPTIMIZATION_TYPE, '0')\n",
    "q = strategy_map.get(QUERY_STRATEGY, 'T')\n",
    "\n",
    "VERSION_STRING = f'SelfIndex-v1.{x}{y}{z}{i}{q}'\n",
    "\n",
    "print(f\"‚úì Version string constructed: {VERSION_STRING}\")\n",
    "print(f\"  x={x} (IndexInfo: {INDEX_TYPE})\")\n",
    "print(f\"  y={y} (DataStore: {STORAGE_TYPE})\")\n",
    "print(f\"  z={z} (Compression: {COMPRESSION_TYPE})\")\n",
    "print(f\"  i={i} (Optimization: {OPTIMIZATION_TYPE})\")\n",
    "print(f\"  q={q} (QueryProc: {QUERY_STRATEGY})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58bb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize IndexBuilder\n",
    "print(f\"\\nInitializing IndexBuilder with version: {VERSION_STRING}\")\n",
    "builder = IndexBuilder(VERSION_STRING)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "for key, value in builder.config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4979e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Build a new index\n",
    "# Uncomment to build from data directory\n",
    "\n",
    "# if DATA_DIR.exists():\n",
    "#     print(f\"Building index from {DATA_DIR}...\")\n",
    "#     index = builder.build_index(DATA_DIR, max_docs=MAX_DOCS)\n",
    "# else:\n",
    "#     print(f\"‚ö†Ô∏è Data directory not found: {DATA_DIR}\")\n",
    "#     print(\"Please update DATA_DIR in configuration or skip to Option 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ac8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Load an existing index\n",
    "print(f\"\\nAttempting to load existing index: {VERSION_STRING}\")\n",
    "try:\n",
    "    index = builder.load_index()\n",
    "    print(f\"‚úì Index loaded successfully\")\n",
    "    print(f\"  Number of documents: {index.num_docs}\")\n",
    "    print(f\"  Number of unique terms: {len(index.index)}\")\n",
    "    print(f\"  Average document length: {index.avg_doc_length:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Failed to load index: {e}\")\n",
    "    print(f\"\\nTo build a new index:\")\n",
    "    print(f\"1. Ensure data directory exists at: {DATA_DIR}\")\n",
    "    print(f\"2. Uncomment 'Option 1: Build a new index' cell above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize query processor\n",
    "print(f\"\\nInitializing QueryProcessor...\")\n",
    "try:\n",
    "    qp = builder.get_query_processor()\n",
    "    print(f\"‚úì Query processor initialized successfully\")\n",
    "    print(f\"  Strategy: {builder.config['query_proc']}\")\n",
    "    print(f\"  Optimization: {builder.config['optimization']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Failed to initialize query processor: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0c56a1",
   "metadata": {},
   "source": [
    "## Section 5: Execute Queries and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple query\n",
    "test_query = \"technology innovation\"\n",
    "print(f\"\\nExecuting test query: '{test_query}'\")\n",
    "print(f\"Top {DEFAULT_TOP_K} results:\\n\")\n",
    "\n",
    "try:\n",
    "    results = qp.process_ranked_query(test_query, top_k=DEFAULT_TOP_K)\n",
    "    \n",
    "    if results:\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            print(f\"{i}. {doc['title'][:60]}...\" if len(doc['title']) > 60 else f\"{i}. {doc['title']}\")\n",
    "            print(f\"   Author: {doc['author']}\")\n",
    "            print(f\"   Published: {doc['published']}\")\n",
    "            print(f\"   Score: {doc['score']:.4f}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Query execution failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean query example\n",
    "boolean_query = '\"artificial\" AND \"intelligence\"'\n",
    "print(f\"\\nExecuting boolean query: {boolean_query}\")\n",
    "\n",
    "try:\n",
    "    result_docs = qp.process_boolean_query(boolean_query)\n",
    "    print(f\"Found {len(result_docs)} documents matching the query\")\n",
    "    print(f\"Doc IDs: {sorted(list(result_docs))[:20]}...\" if len(result_docs) > 20 else f\"Doc IDs: {sorted(result_docs)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Boolean query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b72aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple test queries\n",
    "print(f\"\\nGenerating test queries...\")\n",
    "test_queries = TestQueryGenerator.generate_queries(index, num_queries=30)\n",
    "print(f\"‚úì Generated {len(test_queries)} test queries\")\n",
    "print(f\"\\nSample queries:\")\n",
    "for i, query in enumerate(test_queries[:5], 1):\n",
    "    print(f\"  {i}. {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00630083",
   "metadata": {},
   "source": [
    "## Section 6: Generate and Visualize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3cfcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance metrics\n",
    "print(f\"\\nMeasuring performance metrics...\")\n",
    "print(f\"This may take a moment...\\n\")\n",
    "\n",
    "metrics_config = config.get('metrics', {})\n",
    "\n",
    "try:\n",
    "    # Measure latency\n",
    "    latency_reps = metrics_config.get('latency_repetitions', 1)\n",
    "    latency_queries = test_queries[:metrics_config.get('throughput_queries', 25)]\n",
    "    \n",
    "    print(f\"Measuring query latency ({len(latency_queries)} queries, {latency_reps} repetitions)...\")\n",
    "    latency_metrics = MetricsCollector.measure_query_latency(\n",
    "        qp, latency_queries, top_k=DEFAULT_TOP_K, repetitions=latency_reps\n",
    "    )\n",
    "    print(f\"‚úì Latency measurement complete\")\n",
    "    \n",
    "    # Measure throughput\n",
    "    print(f\"Measuring query throughput...\")\n",
    "    throughput = MetricsCollector.measure_throughput(\n",
    "        qp, latency_queries, duration=5, repetitions=1\n",
    "    )\n",
    "    print(f\"‚úì Throughput measurement complete\")\n",
    "    \n",
    "    # Measure memory and index size\n",
    "    print(f\"Measuring memory usage and index size...\")\n",
    "    memory = MetricsCollector.measure_memory()\n",
    "    index_size = MetricsCollector.measure_index_size(VERSION_STRING, builder.config['datastore'])\n",
    "    print(f\"‚úì Memory and size measurement complete\")\n",
    "    \n",
    "    # Compile metrics\n",
    "    metrics = {\n",
    "        'latency': latency_metrics,\n",
    "        'throughput': throughput,\n",
    "        'memory': memory,\n",
    "        'index_size': index_size\n",
    "    }\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Metrics collection failed: {e}\")\n",
    "    metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab94667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics report\n",
    "if metrics:\n",
    "    Reporter.print_metrics_report(VERSION_STRING, metrics)\n",
    "else:\n",
    "    print(\"No metrics available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87eddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of latency metrics\n",
    "if 'latency' in metrics:\n",
    "    latency = metrics['latency']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Latency distribution\n",
    "    ax1 = axes[0]\n",
    "    metrics_to_plot = ['mean', 'median', 'p95', 'p99', 'min', 'max']\n",
    "    values = [latency.get(m, 0) for m in metrics_to_plot]\n",
    "    colors = ['green', 'blue', 'orange', 'red', 'lightgreen', 'lightcoral']\n",
    "    \n",
    "    bars = ax1.bar(metrics_to_plot, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_ylabel('Latency (ms)', fontsize=12)\n",
    "    ax1.set_title('Query Latency Metrics', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Summary statistics table\n",
    "    ax2 = axes[1]\n",
    "    ax2.axis('tight')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    summary_data = [\n",
    "        ['Metric', 'Value'],\n",
    "        ['Mean Latency (ms)', f\"{latency.get('mean', 0):.4f}\"],\n",
    "        ['Median Latency (ms)', f\"{latency.get('median', 0):.4f}\"],\n",
    "        ['P95 Latency (ms)', f\"{latency.get('p95', 0):.4f}\"],\n",
    "        ['P99 Latency (ms)', f\"{latency.get('p99', 0):.4f}\"],\n",
    "        ['Std Dev (ms)', f\"{latency.get('std', 0):.4f}\"],\n",
    "        ['Throughput (q/s)', f\"{metrics.get('throughput', 0):.2f}\"],\n",
    "        ['Memory (MB)', f\"{metrics.get('memory', 0):.2f}\"],\n",
    "        ['Index Size (MB)', f\"{metrics.get('index_size', 0):.2f}\"]\n",
    "    ]\n",
    "    \n",
    "    table = ax2.table(cellText=summary_data, cellLoc='left', loc='center',\n",
    "                     colWidths=[0.6, 0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Style header row\n",
    "    for i in range(2):\n",
    "        table[(0, i)].set_facecolor('#40466e')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    plt.suptitle(f'Performance Metrics - {VERSION_STRING}', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    os.makedirs('plot', exist_ok=True)\n",
    "    plot_path = os.path.join('plot', f'metrics_{VERSION_STRING}.png')\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"‚úì Plot saved to {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No latency metrics available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de9dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary report\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"INFERENCE & EVALUATION SUMMARY REPORT\")\n",
    "print(f\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìå Configuration\")\n",
    "print(f\"  Version: {VERSION_STRING}\")\n",
    "print(f\"  Index Type: {INDEX_TYPE}\")\n",
    "print(f\"  Storage: {STORAGE_TYPE}\")\n",
    "print(f\"  Compression: {COMPRESSION_TYPE}\")\n",
    "print(f\"  Optimization: {OPTIMIZATION_TYPE}\")\n",
    "print(f\"  Query Strategy: {QUERY_STRATEGY}\")\n",
    "\n",
    "print(f\"\\nüìä Index Statistics\")\n",
    "if index:\n",
    "    print(f\"  Total Documents: {index.num_docs}\")\n",
    "    print(f\"  Unique Terms: {len(index.index)}\")\n",
    "    print(f\"  Avg Doc Length: {index.avg_doc_length:.2f}\")\n",
    "    print(f\"  Avg Docs per Term: {np.mean([info['df'] for info in index.index.values() if 'df' in info]):.2f}\")\n",
    "\n",
    "print(f\"\\n‚ö° Performance Metrics\")\n",
    "if metrics:\n",
    "    print(f\"  Mean Latency: {metrics['latency'].get('mean', 0):.4f} ms\")\n",
    "    print(f\"  P95 Latency: {metrics['latency'].get('p95', 0):.4f} ms\")\n",
    "    print(f\"  Throughput: {metrics.get('throughput', 0):.2f} queries/second\")\n",
    "    print(f\"  Memory Usage: {metrics.get('memory', 0):.2f} MB\")\n",
    "    print(f\"  Index Size: {metrics.get('index_size', 0):.2f} MB\")\n",
    "\n",
    "print(f\"\\nüîç Query Examples Tested\")\n",
    "for i, query in enumerate(test_queries[:3], 1):\n",
    "    print(f\"  {i}. {query}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ec439",
   "metadata": {},
   "source": [
    "## Additional: Run Multiple Benchmark Configurations\n",
    "\n",
    "Uncomment the cells below to run benchmarks with different index configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a3539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Benchmark different index configurations\n",
    "# configurations = [\n",
    "#     ('SelfIndex-v1.1110T', 'Boolean + Custom'),\n",
    "#     ('SelfIndex-v1.2110T', 'WordCount + Custom'),\n",
    "#     ('SelfIndex-v1.3110T', 'TF-IDF + Custom'),\n",
    "# ]\n",
    "\n",
    "# benchmark_results = {}\n",
    "\n",
    "# for version, description in configurations:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Benchmarking: {description}\")\n",
    "#     print(f\"Version: {version}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     \n",
    "#     try:\n",
    "#         builder = IndexBuilder(version)\n",
    "#         index = builder.load_index()  # or build_index()\n",
    "#         qp = builder.get_query_processor()\n",
    "#         \n",
    "#         queries = TestQueryGenerator.generate_queries(index, num_queries=20)\n",
    "#         \n",
    "#         benchmark_results[version] = {\n",
    "#             'latency': MetricsCollector.measure_query_latency(qp, queries[:15]),\n",
    "#             'throughput': MetricsCollector.measure_throughput(qp, queries[:15]),\n",
    "#             'memory': MetricsCollector.measure_memory(),\n",
    "#             'index_size': MetricsCollector.measure_index_size(version, builder.config['datastore'])\n",
    "#         }\n",
    "#         print(\"‚úì Benchmark complete\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚úó Benchmark failed: {e}\")\n",
    "\n",
    "# # Compare results\n",
    "# Reporter.compare_metrics(benchmark_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
